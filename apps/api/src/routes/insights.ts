import { FastifyInstance, FastifyReply, FastifyRequest } from 'fastify'
import { supabase } from '../lib/supabase.js'

// OpenRouter configuration
const openrouter = {
  apiKey: process.env.OPENROUTER_API_KEY || process.env.OPENAI_API_KEY,
  baseURL: 'https://openrouter.ai/api/v1',
}

interface GenerateInsightsRequest {
  Body: {
    period?: 'daily' | 'weekly' | 'monthly'
    topic?: string
  }
}

export async function insightsRoutes(fastify: FastifyInstance) {
  
  // Get insights for the organization
  fastify.get('/insights', {
    // preHandler: [fastify.requireAuth]
  }, async (request: FastifyRequest, reply: FastifyReply) => {
    try {
      // const user = request.user!
      
      const { data: insights, error } = await supabase
        .from('insights')
        .select('*')
        // .eq('org_id', user.orgId)
        .order('created_at', { ascending: false })
        .limit(20)
      
      if (error) {
        throw error
      }
      
      return reply.send({ insights: insights || [] })
      
    } catch (error) {
      request.log.error(error, 'Error fetching insights')
      return reply.code(500).send({ error: 'Failed to fetch insights' })
    }
  })

  // Get specific insight by ID
  fastify.get('/insights/:id', {
    // preHandler: [fastify.requireAuth]
  }, async (request: FastifyRequest<{ Params: { id: string } }>, reply: FastifyReply) => {
    try {
      // const user = request.user!
      const { id } = request.params

      const { data: insight, error } = await supabase
        .from('insights')
        .select('*')
        .eq('id', id)
        // .eq('org_id', user.orgId)
        .single()
      
      if (error) {
        if (error.code === 'PGRST116') {
          return reply.code(404).send({ error: 'Insight not found' })
        }
        throw error
      }
      
      return reply.send({ insight })
      
    } catch (error) {
      request.log.error(error, 'Error fetching insight')
      return reply.code(500).send({ error: 'Failed to fetch insight' })
    }
  })

  // Generate insights manually
  fastify.post<{ Body: GenerateInsightsRequest['Body'] }>('/insights/generate', {
    // preHandler: [fastify.requireAuth]
  }, async (request: FastifyRequest<{ Body: GenerateInsightsRequest['Body'] }>, reply: FastifyReply) => {
    try {
      // const user = request.user!
      const { period = 'daily', topic } = request.body

      // TODO: Add to insights generation queue
      // For now, create a mock insight
      const mockInsight = {
          // org_id: user.orgId,
        period,
        ts: new Date().toISOString(),
        topic: topic || 'General Performance',
        summary: 'Mock insight generated for testing. In production, this would be generated by LLM analysis of your metrics data.',
        details_json: {
          metrics_analyzed: ['spotify_followers', 'monthly_listeners'],
          time_range: '7 days',
          key_findings: [
            'Follower growth increased by 12% week-over-week',
            'Monthly listener engagement showing positive trend'
          ],
          recommendations: [
            'Continue current content strategy',
            'Consider expanding playlist collaborations'
          ]
        },
        source_refs: ['metrics_2025_01', 'vector_search_results'],
        model: 'gpt-4-turbo'
      }

      const { data: insight, error } = await supabase
        .from('insights')
        .insert(mockInsight)
        .select()
        .single()

      if (error) {
        throw error
      }

      // TODO: Emit webhook event for n8n
      request.log.info({ insight_id: insight.id }, 'Insight generated successfully')
      
      return reply.send({
        message: 'Insight generated successfully',
        insight
      })
      
    } catch (error) {
      request.log.error(error, 'Error generating insight')
      return reply.code(500).send({ error: 'Failed to generate insight' })
    }
  })

  // Vector search endpoint for RAG
  fastify.post('/insights/search', {
    // preHandler: [fastify.requireAuth]
  }, async (request: FastifyRequest<{ Body: { query: string, limit?: number } }>, reply: FastifyReply) => {
    try {
      // const user = request.user!
      const { query, limit = 5 } = request.body

      if (!query) {
        return reply.code(400).send({ error: 'Query is required' })
      }

      // TODO: Implement actual vector search using pgvector
      // For now, return mock results
      const mockResults = [
        {
          chunk_id: '123e4567-e89b-12d3-a456-426614174000',
          content: 'Spotify follower growth strategies typically show best results when...',
          similarity: 0.85,
          document_title: 'Music Marketing Best Practices'
        },
        {
          chunk_id: '123e4567-e89b-12d3-a456-426614174001', 
          content: 'Weekly performance metrics indicate that consistent posting...',
          similarity: 0.78,
          document_title: 'Platform Analytics Guide'
        }
      ]

      return reply.send({
        query,
        results: mockResults.slice(0, limit)
      })

    } catch (error) {
      request.log.error(error, 'Error performing vector search')
      return reply.code(500).send({ error: 'Failed to perform search' })
    }
  })

  // AI-powered semantic search for campaigns
  fastify.post('/ai-search/similar-campaigns', async (request, reply) => {
    try {
      console.log('AI search route called');
      return reply.send({
        message: 'AI search endpoint is working!',
        query: 'test query',
        results: [],
        total: 0,
      })
    } catch (error) {
      console.error('AI search error:', error);
      return reply.code(500).send({ error: 'Internal server error' })
    }
  })

  // Generate and store embedding for specific content
  fastify.post('/ai-search/generate-embedding', {
    // preHandler: [fastify.requireAuth]
  }, async (request: FastifyRequest<{
    Body: {
      contentType: string
      contentId: string
      content: string
      metadata?: any
    }
  }>, reply: FastifyReply) => {
    try {
      const { contentType, contentId, content, metadata = {} } = request.body

      if (!contentType || !contentId || !content) {
        return reply.code(400).send({
          error: 'contentType, contentId, and content are required'
        })
      }

      // Generate embedding using OpenRouter
      const embeddingResponse = await fetch(`${openrouter.baseURL}/embeddings`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${openrouter.apiKey}`,
          'HTTP-Referer': 'https://api.artistinfluence.com',
          'X-Title': 'ARTi Platform API',
        },
        body: JSON.stringify({
          model: 'openai/text-embedding-ada-002',
          input: content,
          encoding_format: 'float',
        }),
      })

      if (!embeddingResponse.ok) {
        const errorData = await embeddingResponse.text()
        throw new Error(`OpenRouter API error (${embeddingResponse.status}): ${errorData}`)
      }

      const embeddingData = await embeddingResponse.json() as any
      const embedding = embeddingData.data?.[0]?.embedding

      // Store in database
      const { data, error } = await supabase
        .from('content_embeddings')
        .upsert({
          content_type: contentType,
          content_id: contentId,
          content: content,
          embedding: embedding,
          metadata: metadata,
        })
        .select()
        .single()

      if (error) {
        request.log.error(error, 'Error storing embedding')
        return reply.code(500).send({ error: 'Failed to store embedding' })
      }

      return reply.send({
        success: true,
        embeddingId: data.id,
        contentType,
        contentId,
      })

    } catch (error) {
      request.log.error(error, 'Error generating embedding')
      return reply.code(500).send({ error: 'Internal server error' })
    }
  })

  // Generate embeddings for all content of a specific type
  fastify.post('/ai-search/generate-all-embeddings', {
    // preHandler: [fastify.requireAuth]
  }, async (request: FastifyRequest<{
    Body: {
      contentType: string
    }
  }>, reply: FastifyReply) => {
    try {
      const { contentType } = request.body

      if (!contentType) {
        return reply.code(400).send({ error: 'contentType is required' })
      }

      // This would typically trigger the generate-embeddings.js script
      // For now, we'll do it synchronously (consider using a job queue in production)
      const { exec } = require('child_process')
      const scriptPath = require('path').join(__dirname, '../../scripts/generate-embeddings.js')

      exec(`node ${scriptPath} --generate-all --content-type ${contentType}`, (error: any, stdout: string) => {
        if (error) {
          request.log.error(error, 'Error running embedding generation')
          return reply.code(500).send({ error: 'Failed to generate embeddings' })
        }

        return reply.send({
          success: true,
          message: 'Embedding generation started',
          output: stdout,
        })
      })

    } catch (error) {
      request.log.error(error, 'Error in generate all embeddings')
      return reply.code(500).send({ error: 'Internal server error' })
    }
  })
}
