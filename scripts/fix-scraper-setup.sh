#!/bin/bash

# Fix Spotify Scraper Setup Issues
echo "üîß FIXING SPOTIFY SCRAPER SETUP"
echo "==============================="

cd /root/arti-marketing-ops

# Fix 1: Install missing Python packages
echo "üêç Installing missing Python packages..."
apt update
apt install -y python3-venv python3-pip

# Fix 2: Recreate Python virtual environment
echo "üì¶ Creating Python virtual environment..."
cd spotify_scraper

# Remove failed venv if it exists
rm -rf venv

# Create new virtual environment
python3 -m venv venv

# Activate and install dependencies
source venv/bin/activate

# Install basic dependencies
pip install --upgrade pip

# Install scraper requirements
if [ -f "requirements.txt" ]; then
    pip install -r requirements.txt
else
    echo "Installing default dependencies..."
    pip install playwright beautifulsoup4 asyncio aiofiles
fi

# Install Playwright browsers
playwright install chromium

echo "‚úÖ Python environment fixed"

# Fix 3: Ensure management script exists and is executable
echo "üîß Creating management script..."
cd /root/arti-marketing-ops

cat > scripts/manage-scraper.sh << 'EOF'
#!/bin/bash

# Manage Spotify Scraper Service
SCRAPER_PATH="/root/arti-marketing-ops/spotify_scraper"

case "$1" in
    start)
        echo "üéµ Starting Spotify scraper environment..."
        cd "$SCRAPER_PATH"
        source venv/bin/activate
        echo "‚úÖ Scraper environment ready"
        echo "Python: $(python --version)"
        echo "Playwright: $(playwright --version || echo 'Not available')"
        ;;
    stop)
        echo "üõë Stopping any running scraper processes..."
        pkill -f "python.*run_multi_scraper" || echo "No processes to stop"
        echo "‚úÖ Scraper processes stopped"
        ;;
    status)
        echo "üìä Scraper Status:"
        echo "==================="
        if [ -d "$SCRAPER_PATH/venv" ]; then
            echo "Virtual environment: ‚úÖ Available"
            cd "$SCRAPER_PATH"
            source venv/bin/activate
            echo "Python: $(python --version)"
            echo "Playwright: $(playwright --version 2>/dev/null || echo '‚ùå Not available')"
        else
            echo "Virtual environment: ‚ùå Missing"
        fi
        echo "Active jobs: $(pgrep -f "python.*run_multi_scraper" | wc -l)"
        echo "Data directory: $([ -d "$SCRAPER_PATH/data" ] && echo "‚úÖ Available" || echo "‚ùå Missing")"
        ;;
    test)
        echo "üß™ Testing scraper setup..."
        cd "$SCRAPER_PATH"
        source venv/bin/activate
        
        echo "Testing Python modules..."
        python -c "import sys; print(f'‚úÖ Python {sys.version}')" || echo "‚ùå Python issue"
        python -c "import playwright; print('‚úÖ Playwright available')" || echo "‚ùå Playwright not available"
        python -c "import json; print('‚úÖ JSON available')" || echo "‚ùå JSON issue"
        python -c "import asyncio; print('‚úÖ Asyncio available')" || echo "‚ùå Asyncio issue"
        
        echo "Testing file structure..."
        [ -f "run_multi_scraper_config.py" ] && echo "‚úÖ Main scraper script found" || echo "‚ùå Main scraper script missing"
        [ -d "data" ] && echo "‚úÖ Data directory found" || echo "‚ùå Data directory missing"
        
        echo "‚úÖ Test complete"
        ;;
    clean)
        echo "üßπ Cleaning old scraper data..."
        find "$SCRAPER_PATH/data" -name "*.json" -mtime +7 -delete 2>/dev/null || echo "No old files to clean"
        echo "‚úÖ Cleanup complete"
        ;;
    install)
        echo "üîß Installing/reinstalling scraper dependencies..."
        cd "$SCRAPER_PATH"
        source venv/bin/activate
        pip install --upgrade pip
        if [ -f "requirements.txt" ]; then
            pip install -r requirements.txt
        else
            pip install playwright beautifulsoup4 asyncio aiofiles
        fi
        playwright install chromium
        echo "‚úÖ Installation complete"
        ;;
    *)
        echo "Usage: $0 {start|stop|status|test|clean|install}"
        echo ""
        echo "Commands:"
        echo "  start   - Activate scraper environment"
        echo "  stop    - Stop running scraper processes"
        echo "  status  - Show scraper status and health"
        echo "  test    - Run comprehensive tests"
        echo "  clean   - Remove old data files"
        echo "  install - Install/reinstall dependencies"
        exit 1
        ;;
esac
EOF

chmod +x scripts/manage-scraper.sh

echo "‚úÖ Management script created"

# Fix 4: Update Docker Compose with environment variables
echo "üê≥ Updating Docker Compose..."

# Check if the API service exists and add environment variables
if grep -q "api:" docker-compose.supabase-project.yml; then
    echo "Adding scraper environment variables to API service..."
    
    # Create a backup
    cp docker-compose.supabase-project.yml docker-compose.supabase-project.yml.backup
    
    # Add environment variables to API service
    sed -i '/api:/,/^  [a-zA-Z]/ {
        /environment:/,/^  [a-zA-Z]/ {
            /^  [a-zA-Z]/i\
      - SPOTIFY_SCRAPER_PATH=/root/arti-marketing-ops/spotify_scraper\
      - SPOTIFY_SCRAPER_OUTPUT_PATH=/root/arti-marketing-ops/spotify_scraper/data
        }
    }' docker-compose.supabase-project.yml
    
    echo "‚úÖ Docker Compose updated"
else
    echo "‚ö†Ô∏è  API service not found in docker-compose.supabase-project.yml"
fi

# Fix 5: Test everything
echo "üß™ Running comprehensive tests..."

./scripts/manage-scraper.sh status
./scripts/manage-scraper.sh test

# Fix 6: Try to restart backend API
echo "üîÑ Restarting backend services..."

# Find the correct container name
API_CONTAINER=$(docker ps --format "table {{.Names}}" | grep -E "(api|backend)" | head -n1)

if [ ! -z "$API_CONTAINER" ]; then
    echo "Restarting container: $API_CONTAINER"
    docker restart "$API_CONTAINER"
    sleep 3
else
    echo "API container not found, restarting all services..."
    docker-compose -f docker-compose.supabase-project.yml restart
    sleep 5
fi

# Test API endpoint
echo "üåê Testing API integration..."
API_HEALTH=$(curl -s http://localhost:3002/api/providers/spotify/health 2>/dev/null || echo "ERROR")

if [[ "$API_HEALTH" == *"health"* ]]; then
    echo "‚úÖ API integration working"
    echo "Response: $API_HEALTH"
else
    echo "‚ö†Ô∏è  API integration test failed"
    echo "Response: $API_HEALTH"
    echo "Checking if services are running..."
    docker ps --format "table {{.Names}}\t{{.Status}}" | grep -E "(api|backend)"
fi

echo ""
echo "üéâ SCRAPER SETUP FIXES COMPLETE!"
echo "================================"
echo ""
echo "üìä Final Status:"
./scripts/manage-scraper.sh status

echo ""
echo "üîß Next Steps:"
echo "‚Ä¢ Test scraper: ./scripts/manage-scraper.sh test"
echo "‚Ä¢ Check API: curl http://localhost:3002/api/providers/spotify/health"
echo "‚Ä¢ Access frontend: https://app.artistinfluence.com/admin/integrations"
echo ""
EOF

chmod +x scripts/fix-scraper-setup.sh
