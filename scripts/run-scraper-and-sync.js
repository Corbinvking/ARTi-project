#!/usr/bin/env node

/**
 * One-Click Spotify Scraper + Local Supabase Sync
 * 
 * This script:
 * 1. Runs the Python scraper locally
 * 2. Reads the scraped data
 * 3. Automatically syncs to local Supabase instance
 * 4. Shows you the results
 */

const { createClient } = require('@supabase/supabase-js');
const { spawn } = require('child_process');
const fs = require('fs');
const path = require('path');

// Local Supabase configuration
const supabaseUrl = 'http://127.0.0.1:54321';
const supabaseKey = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZS1kZW1vIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImV4cCI6MTk4MzgxMjk5Nn0.EGIM96RAZx35lJzdJsyH-qQwv8Hdp7fsn3W0YpN81IU';

const supabase = createClient(supabaseUrl, supabaseKey);

async function runScraperAndSync() {
  console.log('ğŸš€ One-Click Spotify Scraper + Local Supabase Sync\n');
  console.log('This will:');
  console.log('  1. ğŸµ Run the Python scraper locally');
  console.log('  2. ğŸ“Š Read the scraped data');
  console.log('  3. ğŸ’¾ Sync to your local Supabase instance');
  console.log('  4. ğŸ“‹ Show you the results\n');

  try {
    // Step 1: Check if local Supabase is running
    console.log('1ï¸âƒ£ Checking local Supabase connection...');
    const { data: testData, error: testError } = await supabase
      .from('scraped_data')
      .select('id')
      .limit(1);

    if (testError) {
      console.error('âŒ Local Supabase not running or scraped_data table not found');
      console.error('   Please run: npx supabase start');
      return;
    }

    console.log('âœ… Local Supabase is running and accessible');

    // Step 2: Check if scraper directory exists
    console.log('\n2ï¸âƒ£ Checking scraper setup...');
    const scraperDir = path.join(__dirname, '../spotify_scraper');
    
    if (!fs.existsSync(scraperDir)) {
      console.error('âŒ Spotify scraper directory not found:', scraperDir);
      return;
    }

    const configFile = path.join(scraperDir, 'songs_config.json');
    if (!fs.existsSync(configFile)) {
      console.error('âŒ songs_config.json not found');
      return;
    }

    console.log('âœ… Scraper directory and config found');

    // Step 3: Run the Python scraper
    console.log('\n3ï¸âƒ£ Running Python scraper...');
    console.log('   ğŸ­ This will open a browser window - you can watch it work!');
    
    const scraperProcess = spawn('python', ['run_multi_scraper_config.py'], {
      cwd: scraperDir,
      stdio: 'inherit',
      shell: true
    });

    // Wait for scraper to complete
    await new Promise((resolve, reject) => {
      scraperProcess.on('close', (code) => {
        if (code === 0) {
          console.log('âœ… Scraper completed successfully');
          resolve();
        } else {
          console.error(`âŒ Scraper failed with code ${code}`);
          reject(new Error(`Scraper failed with code ${code}`));
        }
      });

      scraperProcess.on('error', (error) => {
        console.error('âŒ Error running scraper:', error.message);
        reject(error);
      });
    });

    // Step 4: Read and process scraped data
    console.log('\n4ï¸âƒ£ Processing scraped data...');
    const dataDir = path.join(scraperDir, 'data');
    
    if (!fs.existsSync(dataDir)) {
      console.error('âŒ Data directory not found');
      return;
    }

    // Get the most recent song files
    const songFiles = fs.readdirSync(dataDir)
      .filter(file => file.startsWith('song_') && file.endsWith('.json'))
      .sort()
      .slice(-10); // Get the 10 most recent files

    if (songFiles.length === 0) {
      console.log('âš ï¸  No new scraped data files found');
      return;
    }

    console.log(`âœ… Found ${songFiles.length} scraped data files`);

    // Step 5: Create or get default organization
    console.log('\n5ï¸âƒ£ Setting up organization...');
    const defaultOrgId = '00000000-0000-0000-0000-000000000001';
    
    const { data: org, error: orgError } = await supabase
      .from('orgs')
      .upsert({
        id: defaultOrgId,
        name: 'Local Scraper Data',
        slug: 'local-scraper-data'
      })
      .select()
      .single();

    if (orgError) {
      console.error('âŒ Error creating org:', orgError.message);
      return;
    }

    console.log('âœ… Organization ready');

    // Step 6: Sync data to local Supabase
    console.log('\n6ï¸âƒ£ Syncing data to local Supabase...');
    let successCount = 0;
    let errorCount = 0;

    for (const file of songFiles) {
      try {
        const filePath = path.join(dataDir, file);
        const fileContent = fs.readFileSync(filePath, 'utf-8');
        const scrapedData = JSON.parse(fileContent);

        // Extract song URL from filename
        const songId = file.match(/song_([^_]+)_/)?.[1];
        const songUrl = `https://artists.spotify.com/song/${songId}`;

        // Parse the scraped data
        const songData = scrapedData.song || scrapedData.data?.song || scrapedData;
        
        if (!songData) {
          console.log(`âš ï¸  Skipping ${file}: No song data found`);
          continue;
        }

        // Prepare data for insertion
        const insertData = {
          org_id: defaultOrgId,
          platform: 'spotify',
          song_url: songUrl,
          artist_name: songData.artist_name || songData.artist,
          song_title: songData.song_title || songData.title || songData.name,
          album_name: songData.album_name || songData.album,
          release_date: songData.release_date ? new Date(songData.release_date) : null,
          duration_ms: songData.duration_ms || songData.duration,
          popularity: songData.popularity,
          explicit: songData.explicit,
          genres: songData.genres || [],
          external_urls: songData.external_urls || {},
          raw_data: scrapedData,
          scraped_at: new Date()
        };

        // Insert into local Supabase
        const { error: insertError } = await supabase
          .from('scraped_data')
          .insert(insertData);

        if (insertError) {
          console.error(`âŒ Error inserting ${file}:`, insertError.message);
          errorCount++;
        } else {
          console.log(`âœ… Synced: ${insertData.artist_name} - ${insertData.song_title}`);
          successCount++;
        }

      } catch (error) {
        console.error(`âŒ Error processing ${file}:`, error.message);
        errorCount++;
      }
    }

    // Step 7: Show results
    console.log('\nğŸ“Š Sync Results:');
    console.log(`   âœ… Successfully synced: ${successCount} songs`);
    console.log(`   âŒ Errors: ${errorCount} songs`);
    console.log(`   ğŸ“ Files processed: ${songFiles.length}`);

    // Step 8: Show recent data
    console.log('\n7ï¸âƒ£ Recent scraped data in local Supabase:');
    const { data: recentData, error: recentError } = await supabase
      .from('scraped_data')
      .select('*')
      .eq('org_id', defaultOrgId)
      .order('created_at', { ascending: false })
      .limit(5);

    if (recentError) {
      console.error('âŒ Error fetching recent data:', recentError.message);
      return;
    }

    if (recentData.length > 0) {
      console.log('\nğŸµ Latest scraped songs:');
      recentData.forEach((record, index) => {
        console.log(`   ${index + 1}. ${record.artist_name} - ${record.song_title}`);
        console.log(`      ğŸ“Š Popularity: ${record.popularity || 'N/A'}`);
        console.log(`      â±ï¸  Duration: ${record.duration_ms ? Math.round(record.duration_ms / 1000) + 's' : 'N/A'}`);
        console.log(`      ğŸ“… Scraped: ${new Date(record.scraped_at).toLocaleString()}`);
        console.log('');
      });
    }

    console.log('ğŸ‰ One-click scraper sync completed successfully!');
    console.log('\nğŸ“‹ What you can do now:');
    console.log('   â€¢ View data in Supabase Studio: http://localhost:54323');
    console.log('   â€¢ Query data via API: http://localhost:54321');
    console.log('   â€¢ Run this script again to scrape more songs');
    console.log('   â€¢ Push data to production when ready');

  } catch (error) {
    console.error('âŒ Script failed:', error.message);
    console.error('Stack trace:', error.stack);
  }
}

// Run the script
runScraperAndSync().catch(console.error);
