# ğŸ¯ 1-Click Complete Pipeline Guide

**From CSV to Production in ONE Command!**

---

## âš¡ Quick Start

### **Windows Users**

**Double-click this file:**
```
RUN-COMPLETE-PIPELINE.bat
```

Or run in terminal:
```powershell
.\RUN-COMPLETE-PIPELINE.ps1
```

### **All Users**

```bash
python scripts/run_complete_pipeline.py
```

---

## ğŸ¬ What Happens When You Run It

### **Stage 1: URL Collection** (~15-20 min)
- âœ… Reads your `Spotify Playlisting-Active Campaigns.csv`
- âœ… Navigates to Spotify for Artists Roster
- âœ… Searches for each artist
- âœ… Extracts SFA URLs for all songs
- ğŸ“ **Output**: `roster_scraper/data/sfa-urls-simple_*.txt`

### **Stage 2: Stream Data Scraping** (~2-3 hours)
- âœ… Reads collected SFA URLs
- âœ… Logs into Spotify for Artists (one-time login)
- âœ… Scrapes playlist data for each song
- âœ… Gets 3 time ranges: 28 days, 7 days, 12 months
- ğŸ“ **Output**: `spotify_scraper/data/roster_*.json` (79 files)

### **Stage 3: Local Database Import** (~30 sec)
- âœ… Reads all scraped JSON files
- âœ… Matches track IDs to campaigns
- âœ… Creates playlist records in local database
- ğŸ’¾ **Result**: ~1,000+ records in `campaign_playlists` table

### **Stage 4: Local Verification** (~5 sec)
- âœ… Queries local database
- âœ… Counts playlist records
- âœ… Confirms data integrity

### **Stage 5: Production Upload** (~2 min)
- âœ… Uploads all 79 JSON files to production server
- âœ… Uses SCP (secure copy)
- ğŸŒ **Target**: `root@164.90.129.146:~/arti-marketing-ops/`

### **Stage 6: Production Import** (~30 sec)
- âœ… SSHs into production server
- âœ… Runs import script
- âœ… Creates playlist records in production database
- ğŸš€ **Result**: Data live on https://artistinfluence.com

---

## ğŸ“Š Expected Results

### **Local Database**
```
âœ… Data files processed: 79
âœ… Campaigns updated: 28-60
âœ… Playlists processed: 741-1,471
```

### **Production Database**
```
âœ… Data files processed: 79
âœ… Campaigns updated: 60+
âœ… Playlists processed: 1,471+
```

### **Total Time**
- **Full pipeline**: ~3 hours
- **Production deployment**: ~3 minutes

---

## ğŸ›ï¸ Advanced Options

### **Use Custom CSV**
```bash
python scripts/run_complete_pipeline.py --csv "path/to/custom-campaigns.csv"
```

### **Local Only (Test Mode)**
Skip production deployment to test locally first:
```bash
python scripts/run_complete_pipeline.py --local-only
```

Then later, deploy to production:
```bash
python scripts/run_complete_pipeline.py --production-only
```

### **Custom Server**
```bash
python scripts/run_complete_pipeline.py --server-ip "192.168.1.100"
```

---

## ğŸ” Verify Results

### **Check Local UI**
```
http://localhost:3000
â†’ Navigate to Campaigns
â†’ Click on a campaign
â†’ See playlist data
```

### **Check Production UI**
```
https://artistinfluence.com
â†’ Navigate to Campaigns
â†’ Click on a campaign
â†’ See playlist data
```

### **Check Database Directly**

**Local:**
```bash
npx supabase db execute "SELECT COUNT(*) FROM campaign_playlists;"
```

**Production:**
```bash
ssh root@164.90.129.146
cd ~/arti-marketing-ops
npx supabase db execute --linked "SELECT COUNT(*) FROM campaign_playlists;"
```

---

## ğŸ“‹ Prerequisites

### **First Time Setup**

1. **Install Dependencies**
   ```bash
   # Roster scraper
   cd roster_scraper
   pip install -r requirements.txt
   playwright install chromium
   
   # Spotify scraper
   cd ../spotify_scraper
   pip install -r requirements.txt
   
   # Node.js
   cd ..
   npm install
   ```

2. **CSV File**
   - Place `Spotify Playlisting-Active Campaigns.csv` in project root
   - Must have columns: Client, Campaign, URL, SFA, Status

3. **Environment Variables**
   - `.env` file with Supabase credentials
   - Local and production Supabase URLs

4. **SSH Access**
   - SSH key configured for production server
   - Ability to run: `ssh root@164.90.129.146`

---

## ğŸ”„ Weekly Workflow

### **Recommended Schedule**

**Every Sunday at 10 AM:**

1. **Update CSV** with new campaigns (if any)
2. **Run 1-click pipeline**:
   ```
   RUN-COMPLETE-PIPELINE.bat
   ```
3. **Go do something else** for 3 hours
4. **Come back** to updated production data
5. **Verify** in UI

### **Automated Schedule (Optional)**

**Windows Task Scheduler:**
```powershell
# Create task to run every Sunday at 10 AM
$action = New-ScheduledTaskAction -Execute "python" -Argument "scripts\run_complete_pipeline.py" -WorkingDirectory "C:\Users\Admin\Desktop\ARTi-project"
$trigger = New-ScheduledTaskTrigger -Weekly -DaysOfWeek Sunday -At 10am
Register-ScheduledTask -TaskName "StreamDataPipeline" -Action $action -Trigger $trigger
```

---

## ğŸ› Troubleshooting

### **Issue: "No campaigns found"**
- **Fix**: Check CSV file exists and has active campaigns

### **Issue: "Login required"**
- **Fix**: Run manually first time to log in to Spotify for Artists
- Browser will open for you to log in
- Session persists for future runs

### **Issue: "Upload failed"**
- **Fix**: Check SSH access: `ssh root@164.90.129.146`
- Verify server IP is correct

### **Issue: "Import failed on production"**
- **Fix**: SSH in and check logs:
  ```bash
  ssh root@164.90.129.146
  cd ~/arti-marketing-ops
  tail -100 logs/stream_data_workflow.log
  ```

---

## ğŸ“ˆ Performance Tips

### **Speed Up Scraping**
- âœ… Run on fast internet connection
- âœ… Keep browser logged in
- âœ… Run during off-peak hours

### **Reduce Production Time**
- âœ… Use upload method (not full scraping on production)
- âœ… SCP is fast for file transfer
- âœ… Production import is always quick (~30 sec)

### **Optimize Workflow**
- âœ… Test with `--local-only` first
- âœ… Verify data before deploying
- âœ… Keep scraped files for re-deployment

---

## ğŸ‰ Success Criteria

After running the pipeline, you should see:

### **Terminal Output**
```
âœ… Stage 1 (URL Collection):        SUCCESS
âœ… Stage 2 (Stream Scraping):       SUCCESS
âœ… Stage 3 (Local Import):          SUCCESS
âœ… Stage 4 (Local Verification):    SUCCESS
âœ… Stage 5 (Production Upload):     SUCCESS
âœ… Stage 6 (Production Import):     SUCCESS

ğŸŒ Production Deployment Complete!
   Check your production UI: https://artistinfluence.com

ğŸ’¾ Local Database Updated!
   Check your local UI: http://localhost:3000
```

### **UI**
- Campaigns show playlist cards
- Stream counts visible (28d, 7d, 12m)
- Algorithmic playlists separated
- Vendor playlists displayed

### **Database**
- 1,000+ playlist records
- 60+ campaigns with data
- Recent timestamps

---

## ğŸ“ Need Help?

**Quick Diagnostics:**
```bash
# Check local data
npx supabase db execute "SELECT COUNT(*) FROM campaign_playlists;"

# Check production data
ssh root@164.90.129.146 "cd ~/arti-marketing-ops && npx supabase db execute --linked 'SELECT COUNT(*) FROM campaign_playlists;'"

# View logs
cat logs/last_successful_run.txt
```

**Documentation:**
- Full Workflow: `AUTOMATED-STREAM-DATA-WORKFLOW.md`
- Production Guide: `PRODUCTION-DEPLOYMENT-SUCCESS.md`
- Quick Reference: `STREAM-DATA-QUICK-START.md`

---

## ğŸŠ Summary

**One Command. Complete Pipeline. Production Ready.**

```
CSV â†’ URLs â†’ Scraping â†’ Local DB â†’ Production DB â†’ Live UI
```

**Just run:**
```
RUN-COMPLETE-PIPELINE.bat
```

**And you're done!** âœ…

---

**Happy scraping!** ğŸš€

