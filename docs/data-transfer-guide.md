# Data Transfer Guide: Local â†’ Production

## ðŸ—„ï¸ **What Data Carries Over Automatically**

### âœ… **Schema & Structure (Via Migrations)**
Your GitHub repo contains **ALL** the database schema:
- **13 migration files** in `supabase/migrations/`
- **Complete campaigns table** schema (handles all 4 CSV files)
- **User permissions system** (RBAC)
- **Vector embeddings tables** (for LLM features)
- **All indexes and constraints**

### âœ… **Code & Configuration**
- **Frontend application** (React/Next.js)
- **Backend API** (Fastify)
- **Docker configurations**
- **Caddy reverse proxy setup**

---

## âŒ **What Data DOES NOT Carry Over**

### **1. CSV Campaign Data (5,111 records)**
Your **local Supabase** has:
- âœ… 5,111 campaign records from 4 CSV files
- âœ… Spotify: ~3,000 campaigns
- âœ… SoundCloud: ~1,500 campaigns  
- âœ… YouTube: ~500 campaigns
- âœ… Instagram: ~111 campaigns

**Production Supabase** will have:
- âŒ **EMPTY campaigns table** (schema only)

### **2. User Accounts & Permissions**
Your **local Supabase** has:
- âœ… Admin user: admin@arti-demo.com
- âœ… Test users with different roles
- âœ… User permissions configured

**Production Supabase** will have:
- âŒ **NO users** (you'll need to create them)

### **3. Any Custom Data**
- Test insights
- Uploaded documents
- User preferences
- Connected accounts

---

## ðŸ”„ **Data Migration Options**

### **Option 1: Recreate Data (Recommended)**
This is the cleanest approach for production:

#### **A. CSV Data Recreation:**
```bash
# After production deployment, run on your droplet:
cd /root/arti-marketing-ops

# Copy CSV files to production (if not in repo)
# Your CSV files ARE in the GitHub repo, so they'll be available

# Run the data loading script
node scripts/load-all-csv-data.js
```

#### **B. Create Admin User:**
```bash
# Use the create users script
node scripts/create-production-users.js
```

### **Option 2: Export/Import Data**
If you want to preserve existing data:

#### **A. Export from Local:**
```bash
# Export campaigns data
npx supabase db dump --data-only --table campaigns > campaigns_data.sql

# Export users data  
npx supabase db dump --data-only --table auth.users > users_data.sql
npx supabase db dump --data-only --table user_permissions > permissions_data.sql
```

#### **B. Import to Production:**
```bash
# After production setup, import data
psql "your-production-supabase-connection-string" < campaigns_data.sql
```

---

## ðŸ› ï¸ **Recommended Production Setup Steps**

### **Step 1: Deploy Infrastructure**
```bash
# On your droplet (164.90.129.146)
cd /root/arti-marketing-ops
docker compose -f docker-compose.production.yml up -d --build
```

### **Step 2: Run Migrations**
Your production Supabase will automatically run the migrations from `supabase/migrations/` creating the schema.

### **Step 3: Load Campaign Data**
```bash
# The CSV files are in your GitHub repo, so run:
node scripts/load-all-csv-data.js

# This will load all 5,111 campaign records
```

### **Step 4: Create Admin User**
```bash
# Create your production admin user
node scripts/create-production-users.js
```

---

## ðŸ“‹ **Verification Checklist**

After production deployment, verify:

### **Schema Verification:**
- [ ] Database tables exist (campaigns, users, user_permissions, etc.)
- [ ] Indexes are created
- [ ] RLS policies are active

### **Data Verification:**
- [ ] Campaign data loaded (5,111 records)
- [ ] Admin user created
- [ ] User permissions work
- [ ] Authentication flow works

### **Quick Verification Commands:**
```bash
# Check campaign counts
curl https://api.artistinfluence.com/admin/campaigns/count

# Test authentication
curl https://api.artistinfluence.com/healthz

# Check frontend
curl https://app.artistinfluence.com
```

---

## ðŸš¨ **Important Notes**

### **Your CSV Files ARE in the Repo** âœ…
I can see all 4 CSV files in your GitHub repository:
- `IG Seeding-All Campaigns.csv` (35KB)
- `SoundCloud-All Campaigns.csv` (312KB) 
- `Spotify Playlisting-All Campaigns.csv` (218KB)
- `YouTube-All Campaigns.csv` (198KB)

### **Loading Scripts ARE Ready** âœ…
You have multiple data loading scripts:
- `scripts/load-all-csv-data.js` - Main loader
- `scripts/load-all-platform-data.js` - Platform-specific
- `recovery-flexible-csv-loader.js` - Backup loader

### **Migration Files ARE Complete** âœ…
All database schema is in `supabase/migrations/` - this will create the exact same structure in production.

---

## ðŸŽ¯ **Bottom Line**

**What you need to do after deployment:**

1. **Schema**: âœ… Automatic (via migrations)
2. **CSV Data**: ðŸ”„ Run `node scripts/load-all-csv-data.js`
3. **Admin User**: ðŸ”„ Run `node scripts/create-production-users.js`
4. **Test Everything**: ðŸ”„ Verify data loaded correctly

**Total time**: ~5-10 minutes to recreate all data in production.

**The data recreation is actually GOOD** - it ensures clean, consistent production data without any development artifacts or test data pollution.

---

*Your production deployment will have the exact same functionality and data as your local setup, just with a clean, fresh start.*
